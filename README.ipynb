{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sccomp_glm_data_frame_counts import sccomp_glm_data_frame_counts\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>type</th>\n",
       "      <th>phenotype</th>\n",
       "      <th>count</th>\n",
       "      <th>cell_group</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10x_6K</td>\n",
       "      <td>benign</td>\n",
       "      <td>b_cell_macrophage_precursor_or_follicular_LTB_...</td>\n",
       "      <td>42</td>\n",
       "      <td>BM</td>\n",
       "      <td>0.008350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10x_6K</td>\n",
       "      <td>benign</td>\n",
       "      <td>B_cell:immature</td>\n",
       "      <td>361</td>\n",
       "      <td>B1</td>\n",
       "      <td>0.071769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10x_6K</td>\n",
       "      <td>benign</td>\n",
       "      <td>B_cell:immature_IGLC3_IGLC2</td>\n",
       "      <td>57</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.011332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10x_6K</td>\n",
       "      <td>benign</td>\n",
       "      <td>B_cell:Memory_ITM2C_IGHA1_MZB1_JCHAIN</td>\n",
       "      <td>40</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.007952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10x_6K</td>\n",
       "      <td>benign</td>\n",
       "      <td>Dendritic_CD11_CD1_high_mito</td>\n",
       "      <td>75</td>\n",
       "      <td>Dm</td>\n",
       "      <td>0.014911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>SRR7244582</td>\n",
       "      <td>benign</td>\n",
       "      <td>T_cell:CD8+_GZMK_DUSP2_LYAR_CCL5</td>\n",
       "      <td>197</td>\n",
       "      <td>CD8 2</td>\n",
       "      <td>0.060727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>SRR7244582</td>\n",
       "      <td>benign</td>\n",
       "      <td>T_cell:CD8+_non_activated</td>\n",
       "      <td>320</td>\n",
       "      <td>CD8 3</td>\n",
       "      <td>0.098644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>SRR7244582</td>\n",
       "      <td>benign</td>\n",
       "      <td>T_cell:CD8+_PPBP_SAT1</td>\n",
       "      <td>39</td>\n",
       "      <td>CD8 4</td>\n",
       "      <td>0.012022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>SRR7244582</td>\n",
       "      <td>benign</td>\n",
       "      <td>T_cell:CD8+_S100B</td>\n",
       "      <td>88</td>\n",
       "      <td>CD8 5</td>\n",
       "      <td>0.027127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>SRR7244582</td>\n",
       "      <td>benign</td>\n",
       "      <td>T_cell:CD8low_TIMP1_PPBP</td>\n",
       "      <td>107</td>\n",
       "      <td>CD8 6</td>\n",
       "      <td>0.032984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample    type                                          phenotype  \\\n",
       "0        10x_6K  benign  b_cell_macrophage_precursor_or_follicular_LTB_...   \n",
       "1        10x_6K  benign                                    B_cell:immature   \n",
       "2        10x_6K  benign                        B_cell:immature_IGLC3_IGLC2   \n",
       "3        10x_6K  benign              B_cell:Memory_ITM2C_IGHA1_MZB1_JCHAIN   \n",
       "4        10x_6K  benign                       Dendritic_CD11_CD1_high_mito   \n",
       "..          ...     ...                                                ...   \n",
       "715  SRR7244582  benign                   T_cell:CD8+_GZMK_DUSP2_LYAR_CCL5   \n",
       "716  SRR7244582  benign                          T_cell:CD8+_non_activated   \n",
       "717  SRR7244582  benign                              T_cell:CD8+_PPBP_SAT1   \n",
       "718  SRR7244582  benign                                  T_cell:CD8+_S100B   \n",
       "719  SRR7244582  benign                           T_cell:CD8low_TIMP1_PPBP   \n",
       "\n",
       "     count cell_group  proportion  \n",
       "0       42         BM    0.008350  \n",
       "1      361         B1    0.071769  \n",
       "2       57         B2    0.011332  \n",
       "3       40         B3    0.007952  \n",
       "4       75         Dm    0.014911  \n",
       "..     ...        ...         ...  \n",
       "715    197      CD8 2    0.060727  \n",
       "716    320      CD8 3    0.098644  \n",
       "717     39      CD8 4    0.012022  \n",
       "718     88      CD8 5    0.027127  \n",
       "719    107      CD8 6    0.032984  \n",
       "\n",
       "[720 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_obj = pd.read_csv('./data/count_obj.csv')\n",
    "count_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:05:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:05:59 - cmdstanpy - INFO - Chain [2] start processing\n",
      "15:05:59 - cmdstanpy - INFO - Chain [3] start processing\n",
      "15:05:59 - cmdstanpy - INFO - Chain [4] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain [2] method = sample (Default)\n",
      "Chain [2] sample\n",
      "Chain [2] num_samples = 1000 (Default)\n",
      "Chain [2] num_warmup = 1000 (Default)\n",
      "Chain [2] save_warmup = false (Default)\n",
      "Chain [2] thin = 1 (Default)\n",
      "Chain [2] adapt\n",
      "Chain [2] engaged = true (Default)\n",
      "Chain [2] gamma = 0.05 (Default)\n",
      "Chain [2] delta = 0.8 (Default)\n",
      "Chain [2] kappa = 0.75 (Default)\n",
      "Chain [2] t0 = 10 (Default)\n",
      "Chain [2] init_buffer = 75 (Default)\n",
      "Chain [2] term_buffer = 50 (Default)\n",
      "Chain [2] window = 25 (Default)\n",
      "Chain [2] save_metric = false (Default)\n",
      "Chain [2] algorithm = hmc (Default)\n",
      "Chain [2] hmc\n",
      "Chain [2] engine = nuts (Default)\n",
      "Chain [2] nuts\n",
      "Chain [2] max_depth = 10 (Default)\n",
      "Chain [2] metric = diag_e (Default)\n",
      "Chain [2] metric_file =  (Default)\n",
      "Chain [2] stepsize = 1 (Default)\n",
      "Chain [2] stepsize_jitter = 0 (Default)\n",
      "Chain [2] num_chains = 1 (Default)\n",
      "Chain [2] id = 2\n",
      "Chain [2] data\n",
      "Chain [2] file = /tmp/tmp8pfwxjrl/raje6i1a.json\n",
      "Chain [2] init = 2 (Default)\n",
      "Chain [2] random\n",
      "Chain [2] seed = 36474\n",
      "Chain [2] output\n",
      "Chain [2] file = /tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_2.csv\n",
      "Chain [2] diagnostic_file =  (Default)\n",
      "Chain [2] refresh = 100 (Default)\n",
      "Chain [2] sig_figs = -1 (Default)\n",
      "Chain [2] profile_file = profile.csv (Default)\n",
      "Chain [2] save_cmdstan_config = false (Default)\n",
      "Chain [2] num_threads = 1 (Default)\n",
      "Chain [2] \n",
      "Chain [2] \n",
      "Chain [2] Gradient evaluation took 0.000277 seconds\n",
      "Chain [2] 1000 transitions using 10 leapfrog steps per transition would take 2.77 seconds.\n",
      "Chain [2] Adjust your expectations accordingly!\n",
      "Chain [2] \n",
      "Chain [2] \n",
      "Chain [2] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [2] Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Chain [2] Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n",
      "Chain [2] If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "Chain [2] but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "Chain [2] \n",
      "Chain [1] method = sample (Default)\n",
      "Chain [1] sample\n",
      "Chain [1] num_samples = 1000 (Default)\n",
      "Chain [1] num_warmup = 1000 (Default)\n",
      "Chain [1] save_warmup = false (Default)\n",
      "Chain [1] thin = 1 (Default)\n",
      "Chain [1] adapt\n",
      "Chain [1] engaged = true (Default)\n",
      "Chain [1] gamma = 0.05 (Default)\n",
      "Chain [1] delta = 0.8 (Default)\n",
      "Chain [1] kappa = 0.75 (Default)\n",
      "Chain [1] t0 = 10 (Default)\n",
      "Chain [1] init_buffer = 75 (Default)\n",
      "Chain [1] term_buffer = 50 (Default)\n",
      "Chain [1] window = 25 (Default)\n",
      "Chain [1] save_metric = false (Default)\n",
      "Chain [1] algorithm = hmc (Default)\n",
      "Chain [1] hmc\n",
      "Chain [1] engine = nuts (Default)\n",
      "Chain [1] nuts\n",
      "Chain [1] max_depth = 10 (Default)\n",
      "Chain [1] metric = diag_e (Default)\n",
      "Chain [1] metric_file =  (Default)\n",
      "Chain [1] stepsize = 1 (Default)\n",
      "Chain [1] stepsize_jitter = 0 (Default)\n",
      "Chain [1] num_chains = 1 (Default)\n",
      "Chain [1] id = 1 (Default)\n",
      "Chain [1] data\n",
      "Chain [1] file = /tmp/tmp8pfwxjrl/raje6i1a.json\n",
      "Chain [1] init = 2 (Default)\n",
      "Chain [1] random\n",
      "Chain [1] seed = 36474\n",
      "Chain [1] output\n",
      "Chain [1] file = /tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_1.csv\n",
      "Chain [1] diagnostic_file =  (Default)\n",
      "Chain [1] refresh = 100 (Default)\n",
      "Chain [1] sig_figs = -1 (Default)\n",
      "Chain [1] profile_file = profile.csv (Default)\n",
      "Chain [1] save_cmdstan_config = false (Default)\n",
      "Chain [1] num_threads = 1 (Default)\n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] Gradient evaluation took 0.000295 seconds\n",
      "Chain [1] 1000 transitions using 10 leapfrog steps per transition would take 2.95 seconds.\n",
      "Chain [1] Adjust your expectations accordingly!\n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [1] Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Chain [1] Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n",
      "Chain [1] If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "Chain [1] but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "Chain [1] \n",
      "Chain [3] method = sample (Default)\n",
      "Chain [3] sample\n",
      "Chain [3] num_samples = 1000 (Default)\n",
      "Chain [3] num_warmup = 1000 (Default)\n",
      "Chain [3] save_warmup = false (Default)\n",
      "Chain [3] thin = 1 (Default)\n",
      "Chain [3] adapt\n",
      "Chain [3] engaged = true (Default)\n",
      "Chain [3] gamma = 0.05 (Default)\n",
      "Chain [3] delta = 0.8 (Default)\n",
      "Chain [3] kappa = 0.75 (Default)\n",
      "Chain [3] t0 = 10 (Default)\n",
      "Chain [3] init_buffer = 75 (Default)\n",
      "Chain [3] term_buffer = 50 (Default)\n",
      "Chain [3] window = 25 (Default)\n",
      "Chain [3] save_metric = false (Default)\n",
      "Chain [3] algorithm = hmc (Default)\n",
      "Chain [3] hmc\n",
      "Chain [3] engine = nuts (Default)\n",
      "Chain [3] nuts\n",
      "Chain [3] max_depth = 10 (Default)\n",
      "Chain [3] metric = diag_e (Default)\n",
      "Chain [3] metric_file =  (Default)\n",
      "Chain [3] stepsize = 1 (Default)\n",
      "Chain [3] stepsize_jitter = 0 (Default)\n",
      "Chain [3] num_chains = 1 (Default)\n",
      "Chain [3] id = 3\n",
      "Chain [3] data\n",
      "Chain [3] file = /tmp/tmp8pfwxjrl/raje6i1a.json\n",
      "Chain [3] init = 2 (Default)\n",
      "Chain [3] random\n",
      "Chain [3] seed = 36474\n",
      "Chain [3] output\n",
      "Chain [3] file = /tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_3.csv\n",
      "Chain [3] diagnostic_file =  (Default)\n",
      "Chain [3] refresh = 100 (Default)\n",
      "Chain [3] sig_figs = -1 (Default)\n",
      "Chain [3] profile_file = profile.csv (Default)\n",
      "Chain [3] save_cmdstan_config = false (Default)\n",
      "Chain [3] num_threads = 1 (Default)\n",
      "Chain [3] \n",
      "Chain [3] \n",
      "Chain [3] Gradient evaluation took 0.000291 seconds\n",
      "Chain [3] 1000 transitions using 10 leapfrog steps per transition would take 2.91 seconds.\n",
      "Chain [3] Adjust your expectations accordingly!\n",
      "Chain [3] \n",
      "Chain [3] \n",
      "Chain [4] method = sample (Default)\n",
      "Chain [4] sample\n",
      "Chain [4] num_samples = 1000 (Default)\n",
      "Chain [4] num_warmup = 1000 (Default)\n",
      "Chain [4] save_warmup = false (Default)\n",
      "Chain [4] thin = 1 (Default)\n",
      "Chain [4] adapt\n",
      "Chain [4] engaged = true (Default)\n",
      "Chain [4] gamma = 0.05 (Default)\n",
      "Chain [4] delta = 0.8 (Default)\n",
      "Chain [4] kappa = 0.75 (Default)\n",
      "Chain [4] t0 = 10 (Default)\n",
      "Chain [4] init_buffer = 75 (Default)\n",
      "Chain [4] term_buffer = 50 (Default)\n",
      "Chain [4] window = 25 (Default)\n",
      "Chain [4] save_metric = false (Default)\n",
      "Chain [4] algorithm = hmc (Default)\n",
      "Chain [4] hmc\n",
      "Chain [4] engine = nuts (Default)\n",
      "Chain [4] nuts\n",
      "Chain [3] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [4] max_depth = 10 (Default)\n",
      "Chain [4] metric = diag_e (Default)\n",
      "Chain [4] metric_file =  (Default)\n",
      "Chain [4] stepsize = 1 (Default)\n",
      "Chain [4] stepsize_jitter = 0 (Default)\n",
      "Chain [4] num_chains = 1 (Default)\n",
      "Chain [4] id = 4\n",
      "Chain [4] data\n",
      "Chain [4] file = /tmp/tmp8pfwxjrl/raje6i1a.json\n",
      "Chain [4] init = 2 (Default)\n",
      "Chain [4] random\n",
      "Chain [4] seed = 36474\n",
      "Chain [4] output\n",
      "Chain [4] file = /tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_4.csv\n",
      "Chain [4] diagnostic_file =  (Default)\n",
      "Chain [4] refresh = 100 (Default)\n",
      "Chain [4] sig_figs = -1 (Default)\n",
      "Chain [4] profile_file = profile.csv (Default)\n",
      "Chain [4] save_cmdstan_config = false (Default)\n",
      "Chain [4] num_threads = 1 (Default)\n",
      "Chain [4] \n",
      "Chain [4] \n",
      "Chain [4] Gradient evaluation took 0.000314 seconds\n",
      "Chain [4] 1000 transitions using 10 leapfrog steps per transition would take 3.14 seconds.\n",
      "Chain [4] Adjust your expectations accordingly!\n",
      "Chain [4] \n",
      "Chain [4] \n",
      "Chain [3] Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Chain [3] Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n",
      "Chain [3] If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "Chain [3] but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "Chain [3] \n",
      "Chain [4] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [4] Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Chain [4] Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n",
      "Chain [4] If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "Chain [4] but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "Chain [4] \n",
      "Chain [2] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [4] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [1] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [3] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [1] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [2] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [3] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [4] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [1] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [2] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [3] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [4] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [1] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [2] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [3] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [4] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [1] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [2] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [3] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [4] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [2] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [1] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [3] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [4] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [1] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [2] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [3] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [4] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [1] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [2] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [3] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [4] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [2] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [1] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [3] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [4] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [1] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [1] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [2] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [2] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [3] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [3] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [4] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [4] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [1] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [2] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [3] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [4] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [1] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [2] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [3] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [4] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [1] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [2] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [3] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [4] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [1] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [2] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [3] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [4] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [1] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [2] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [3] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [4] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [1] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [2] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [3] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [4] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [1] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [2] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [3] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [4] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [1] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [2] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [3] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [4] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [1] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [2] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [3] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [4] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:06:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:06:04 - cmdstanpy - INFO - Chain [2] done processing\n",
      "15:06:04 - cmdstanpy - INFO - Chain [3] done processing\n",
      "15:06:04 - cmdstanpy - INFO - Chain [4] done processing\n",
      "15:06:04 - cmdstanpy - WARNING - Non-fatal error during sampling:\n",
      "Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n",
      "Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n",
      "Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n",
      "Exception: Exception: beta_binomial_lpmf: First prior sample size parameter[1] is inf, but must be positive finite! (in 'glm_multi_beta_binomial.stan', line 214, column 16 to line 219, column 19) (in 'glm_multi_beta_binomial.stan', line 653, column 3 to line 683, column 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain [1] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain [1] \n",
      "Chain [1] Elapsed Time: 2.503 seconds (Warm-up)\n",
      "Chain [1] 2.247 seconds (Sampling)\n",
      "Chain [1] 4.75 seconds (Total)\n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [2] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain [2] \n",
      "Chain [2] Elapsed Time: 2.509 seconds (Warm-up)\n",
      "Chain [2] 2.253 seconds (Sampling)\n",
      "Chain [2] 4.762 seconds (Total)\n",
      "Chain [2] \n",
      "Chain [2] \n",
      "Chain [3] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain [3] \n",
      "Chain [3] Elapsed Time: 2.578 seconds (Warm-up)\n",
      "Chain [3] 2.247 seconds (Sampling)\n",
      "Chain [3] 4.825 seconds (Total)\n",
      "Chain [3] \n",
      "Chain [3] \n",
      "Chain [4] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain [4] \n",
      "Chain [4] Elapsed Time: 2.582 seconds (Warm-up)\n",
      "Chain [4] 2.255 seconds (Sampling)\n",
      "Chain [4] 4.837 seconds (Total)\n",
      "Chain [4] \n",
      "Chain [4] \n"
     ]
    }
   ],
   "source": [
    "res = sccomp_glm_data_frame_counts(\n",
    "    data = count_obj,\n",
    "    formula_composition = '~ 0 + type', \n",
    "    sample = 'sample',\n",
    "    cell_group = 'cell_group',\n",
    "    count = 'count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit': CmdStanMCMC: model=glm_multi_beta_binomial chains=4['method=sample', 'algorithm=hmc', 'adapt', 'engaged=1']\n",
       "  csv_files:\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_1.csv\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_2.csv\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_3.csv\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_4.csv\n",
       "  output_files:\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_0-stdout.txt\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_1-stdout.txt\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_2-stdout.txt\n",
       " \t/tmp/tmp8pfwxjrl/glm_multi_beta_binomialafm46vrz/glm_multi_beta_binomial-20241119150559_3-stdout.txt,\n",
       " 'model_input': {'N': 20,\n",
       "  'M': 36,\n",
       "  'exposure': array([ 5030,  8082,  2511,  6161,  6877,  2864,  3199,   601,  4784,\n",
       "          7646,  8734,  3495,  2787,  5446, 25804,  2304,  2570,  2910,\n",
       "         10521,  3244]),\n",
       "  'is_proportion': False,\n",
       "  'y': cell_group     B1   B2   B3   BM  CD4 1  CD4 2  CD4 3  CD4 4  CD4 5  CD8 1  \\\n",
       "  sample                                                                       \n",
       "  10x_6K        361   57   40   42    129    178    701      3    147    431   \n",
       "  10x_8K        537  426   79   87    191    310    862     11    232    561   \n",
       "  GSE115189     136   97   21   21     53     72    340      5     50    191   \n",
       "  SCP345_580    266  209   71   20    201    374    748     11    214    609   \n",
       "  SCP345_860    375  292   94   51    178    352    560     12    228    707   \n",
       "  SCP424_pbmc1  163  102   27   17     60    103    176      6     42    551   \n",
       "  SCP424_pbmc2  325  307   70   23     66    129    191      1     74    347   \n",
       "  SCP591         17    9    7    2     10     43     77      2     11     45   \n",
       "  SI-GA-E5       71   68   23   14    103    168    217      2     87    347   \n",
       "  SI-GA-E7      550  275   77   52    152    243    214      7     97   1228   \n",
       "  SI-GA-E8      577  332  106   33    308    752    475     23    238   1322   \n",
       "  SI-GA-G6       94   60   59   19    128    359    205      9    195    396   \n",
       "  SI-GA-G7       31   16   15   11     58    175     15      3     68    270   \n",
       "  SI-GA-G8      134   90   83   23    189    361    239     12    184    658   \n",
       "  SI-GA-G9      796  568  280  209    663   1278    575      7    269   3681   \n",
       "  SI-GA-H1       60   39   23    9    104    327     28      3    149    241   \n",
       "  SI-GA-H3       40    9   10    5     66    233     76      6     95    305   \n",
       "  SI-GA-H4       58   20    6    8     94    288     43      6    142    377   \n",
       "  SRR11038995   473  257  155   37    259    515    170     12    402   1712   \n",
       "  SRR7244582    178  134   43   22    121    250     86      3    150    506   \n",
       "  \n",
       "  cell_group    ...    M7   M8    M9  NK1  NK2  NK3   NKM  TM1  TM2  TM3  \n",
       "  sample        ...                                                       \n",
       "  10x_6K        ...   233   29   226   86   65  117   128  123   19   25  \n",
       "  10x_8K        ...   416   28   251  156   78  285   233  266   40   63  \n",
       "  GSE115189     ...   125   11   134   39   32   49    84   79   29    0  \n",
       "  SCP345_580    ...   186   13   115  126  103  204   182  145   42   46  \n",
       "  SCP345_860    ...   186   22   195  188   80  405   286  116   47    9  \n",
       "  SCP424_pbmc1  ...    95    9    76   51   33  215   115   25    4   17  \n",
       "  SCP424_pbmc2  ...   139   10    52   81   43  125   104   71   12    0  \n",
       "  SCP591        ...    22    3     9    1    8    3    14   40    2   12  \n",
       "  SI-GA-E5      ...   594   80   332   62   35  114   254  197   52    0  \n",
       "  SI-GA-E7      ...   390   53   342  337  149  417   286  118   26    0  \n",
       "  SI-GA-E8      ...   523   93   225  161  157  400   279  215   41    0  \n",
       "  SI-GA-G6      ...   227   35    93   58   42  121    83   72   18    0  \n",
       "  SI-GA-G7      ...   304   85   113   60   35  107   180  105   27    0  \n",
       "  SI-GA-G8      ...   330   55   145   97   51  147   203  122   23    0  \n",
       "  SI-GA-G9      ...  1863  427  1515  337  532  897  1879  733  335    0  \n",
       "  SI-GA-H1      ...   223   48    63   30   17   55   102   80   11    0  \n",
       "  SI-GA-H3      ...   304   39    99   36   41   74   109   88   25    0  \n",
       "  SI-GA-H4      ...   236   41   101   77   29  132   125   71   14    0  \n",
       "  SRR11038995   ...   342   60   183  553  157  824   498  135   75    0  \n",
       "  SRR7244582    ...   135   15    40   75   40  163   132   74    9    0  \n",
       "  \n",
       "  [20 rows x 36 columns],\n",
       "  'y_proportion': Empty DataFrame\n",
       "  Columns: [B1, B2, B3, BM, CD4 1, CD4 2, CD4 3, CD4 4, CD4 5, CD8 1, CD8 2, CD8 3, CD8 4, CD8 5, CD8 6, Dm, Dp, H1, H2, H3, M1, M2, M3, M4, M5, M6, M7, M8, M9, NK1, NK2, NK3, NKM, TM1, TM2, TM3]\n",
       "  Index: []\n",
       "  \n",
       "  [0 rows x 36 columns],\n",
       "  'X':               type[benign]  type[cancer]\n",
       "  sample                                  \n",
       "  10x_6K                 1.0           0.0\n",
       "  10x_8K                 1.0           0.0\n",
       "  GSE115189              1.0           0.0\n",
       "  SCP345_580             1.0           0.0\n",
       "  SCP345_860             1.0           0.0\n",
       "  SCP424_pbmc1           1.0           0.0\n",
       "  SCP424_pbmc2           1.0           0.0\n",
       "  SCP591                 1.0           0.0\n",
       "  SI-GA-E5               0.0           1.0\n",
       "  SI-GA-E7               0.0           1.0\n",
       "  SI-GA-E8               0.0           1.0\n",
       "  SI-GA-G6               0.0           1.0\n",
       "  SI-GA-G7               0.0           1.0\n",
       "  SI-GA-G8               0.0           1.0\n",
       "  SI-GA-G9               0.0           1.0\n",
       "  SI-GA-H1               0.0           1.0\n",
       "  SI-GA-H3               0.0           1.0\n",
       "  SI-GA-H4               0.0           1.0\n",
       "  SRR11038995            1.0           0.0\n",
       "  SRR7244582             1.0           0.0,\n",
       "  'Xa':               Intercept\n",
       "  sample                 \n",
       "  10x_6K              1.0\n",
       "  10x_8K              1.0\n",
       "  GSE115189           1.0\n",
       "  SCP345_580          1.0\n",
       "  SCP345_860          1.0\n",
       "  SCP424_pbmc1        1.0\n",
       "  SCP424_pbmc2        1.0\n",
       "  SCP591              1.0\n",
       "  SI-GA-E5            1.0\n",
       "  SI-GA-E7            1.0\n",
       "  SI-GA-E8            1.0\n",
       "  SI-GA-G6            1.0\n",
       "  SI-GA-G7            1.0\n",
       "  SI-GA-G8            1.0\n",
       "  SI-GA-G9            1.0\n",
       "  SI-GA-H1            1.0\n",
       "  SI-GA-H3            1.0\n",
       "  SI-GA-H4            1.0\n",
       "  SRR11038995         1.0\n",
       "  SRR7244582          1.0,\n",
       "  'XA':    Intercept\n",
       "  0        1.0,\n",
       "  'C': 2,\n",
       "  'A': 1,\n",
       "  'Ar': 1,\n",
       "  'truncation_ajustment': 1.1,\n",
       "  'is_vb': 1,\n",
       "  'bimodal_mean_variability_association': False,\n",
       "  'use_data': True,\n",
       "  'is_random_effect': 0,\n",
       "  'ncol_X_random_eff': [0, 0],\n",
       "  'n_random_eff': 0,\n",
       "  'n_groups': [0, 0],\n",
       "  'X_random_effect': Empty DataFrame\n",
       "  Columns: []\n",
       "  Index: [10x_6K, 10x_8K, GSE115189, SCP345_580, SCP345_860, SCP424_pbmc1, SCP424_pbmc2, SCP591, SI-GA-E5, SI-GA-E7, SI-GA-E8, SI-GA-G6, SI-GA-G7, SI-GA-G8, SI-GA-G9, SI-GA-H1, SI-GA-H3, SI-GA-H4, SRR11038995, SRR7244582],\n",
       "  'X_random_effect_2': Empty DataFrame\n",
       "  Columns: []\n",
       "  Index: [10x_6K, 10x_8K, GSE115189, SCP345_580, SCP345_860, SCP424_pbmc1, SCP424_pbmc2, SCP591, SI-GA-E5, SI-GA-E7, SI-GA-E8, SI-GA-G6, SI-GA-G7, SI-GA-G8, SI-GA-G9, SI-GA-H1, SI-GA-H3, SI-GA-H4, SRR11038995, SRR7244582],\n",
       "  'group_factor_indexes_for_covariance': Empty DataFrame\n",
       "  Columns: []\n",
       "  Index: [],\n",
       "  'group_factor_indexes_for_covariance_2': Empty DataFrame\n",
       "  Columns: []\n",
       "  Index: [],\n",
       "  'how_many_factors_in_random_design': [0, 0],\n",
       "  'grainsize': 1,\n",
       "  'enable_loo': False,\n",
       "  'is_truncated': 0,\n",
       "  'truncation_up':               BM  B1  B2  B3  Dm  H1  H2  H3  TM1  TM2  ...  CD4 3  TM3  \\\n",
       "  10x_6K        -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  10x_8K        -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  GSE115189     -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP345_580    -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP345_860    -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP424_pbmc1  -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP424_pbmc2  -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP591        -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-E5      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-E7      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-E8      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G6      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G7      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G8      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G9      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-H1      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-H3      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-H4      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SRR11038995   -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SRR7244582    -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  \n",
       "                CD4 4  CD4 5  CD8 1  CD8 2  CD8 3  CD8 4  CD8 5  CD8 6  \n",
       "  10x_6K           -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  10x_8K           -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  GSE115189        -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP345_580       -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP345_860       -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP424_pbmc1     -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP424_pbmc2     -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP591           -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-E5         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-E7         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-E8         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G6         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G7         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G8         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G9         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-H1         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-H3         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-H4         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SRR11038995      -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SRR7244582       -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  \n",
       "  [20 rows x 36 columns],\n",
       "  'truncation_down':               BM  B1  B2  B3  Dm  H1  H2  H3  TM1  TM2  ...  CD4 3  TM3  \\\n",
       "  10x_6K        -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  10x_8K        -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  GSE115189     -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP345_580    -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP345_860    -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP424_pbmc1  -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP424_pbmc2  -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SCP591        -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-E5      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-E7      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-E8      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G6      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G7      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G8      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-G9      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-H1      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-H3      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SI-GA-H4      -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SRR11038995   -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  SRR7244582    -1  -1  -1  -1  -1  -1  -1  -1   -1   -1  ...     -1   -1   \n",
       "  \n",
       "                CD4 4  CD4 5  CD8 1  CD8 2  CD8 3  CD8 4  CD8 5  CD8 6  \n",
       "  10x_6K           -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  10x_8K           -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  GSE115189        -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP345_580       -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP345_860       -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP424_pbmc1     -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP424_pbmc2     -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SCP591           -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-E5         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-E7         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-E8         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G6         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G7         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G8         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-G9         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-H1         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-H3         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SI-GA-H4         -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SRR11038995      -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  SRR7244582       -1     -1     -1     -1     -1     -1     -1     -1  \n",
       "  \n",
       "  [20 rows x 36 columns],\n",
       "  'truncation_not_idx': [1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   208,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   212,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   224,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   252,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   272,\n",
       "   273,\n",
       "   274,\n",
       "   275,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   280,\n",
       "   281,\n",
       "   282,\n",
       "   283,\n",
       "   284,\n",
       "   285,\n",
       "   286,\n",
       "   287,\n",
       "   288,\n",
       "   289,\n",
       "   290,\n",
       "   291,\n",
       "   292,\n",
       "   293,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   297,\n",
       "   298,\n",
       "   299,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   303,\n",
       "   304,\n",
       "   305,\n",
       "   306,\n",
       "   307,\n",
       "   308,\n",
       "   309,\n",
       "   310,\n",
       "   311,\n",
       "   312,\n",
       "   313,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   331,\n",
       "   332,\n",
       "   333,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   337,\n",
       "   338,\n",
       "   339,\n",
       "   340,\n",
       "   341,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   345,\n",
       "   346,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   350,\n",
       "   351,\n",
       "   352,\n",
       "   353,\n",
       "   354,\n",
       "   355,\n",
       "   356,\n",
       "   357,\n",
       "   358,\n",
       "   359,\n",
       "   360,\n",
       "   361,\n",
       "   362,\n",
       "   363,\n",
       "   364,\n",
       "   365,\n",
       "   366,\n",
       "   367,\n",
       "   368,\n",
       "   369,\n",
       "   370,\n",
       "   371,\n",
       "   372,\n",
       "   373,\n",
       "   374,\n",
       "   375,\n",
       "   376,\n",
       "   377,\n",
       "   378,\n",
       "   379,\n",
       "   380,\n",
       "   381,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   387,\n",
       "   388,\n",
       "   389,\n",
       "   390,\n",
       "   391,\n",
       "   392,\n",
       "   393,\n",
       "   394,\n",
       "   395,\n",
       "   396,\n",
       "   397,\n",
       "   398,\n",
       "   399,\n",
       "   400,\n",
       "   401,\n",
       "   402,\n",
       "   403,\n",
       "   404,\n",
       "   405,\n",
       "   406,\n",
       "   407,\n",
       "   408,\n",
       "   409,\n",
       "   410,\n",
       "   411,\n",
       "   412,\n",
       "   413,\n",
       "   414,\n",
       "   415,\n",
       "   416,\n",
       "   417,\n",
       "   418,\n",
       "   419,\n",
       "   420,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   424,\n",
       "   425,\n",
       "   426,\n",
       "   427,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   431,\n",
       "   432,\n",
       "   433,\n",
       "   434,\n",
       "   435,\n",
       "   436,\n",
       "   437,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   441,\n",
       "   442,\n",
       "   443,\n",
       "   444,\n",
       "   445,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   449,\n",
       "   450,\n",
       "   451,\n",
       "   452,\n",
       "   453,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   457,\n",
       "   458,\n",
       "   459,\n",
       "   460,\n",
       "   461,\n",
       "   462,\n",
       "   463,\n",
       "   464,\n",
       "   465,\n",
       "   466,\n",
       "   467,\n",
       "   468,\n",
       "   469,\n",
       "   470,\n",
       "   471,\n",
       "   472,\n",
       "   473,\n",
       "   474,\n",
       "   475,\n",
       "   476,\n",
       "   477,\n",
       "   478,\n",
       "   479,\n",
       "   480,\n",
       "   481,\n",
       "   482,\n",
       "   483,\n",
       "   484,\n",
       "   485,\n",
       "   486,\n",
       "   487,\n",
       "   488,\n",
       "   489,\n",
       "   490,\n",
       "   491,\n",
       "   492,\n",
       "   493,\n",
       "   494,\n",
       "   495,\n",
       "   496,\n",
       "   497,\n",
       "   498,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   502,\n",
       "   503,\n",
       "   504,\n",
       "   505,\n",
       "   506,\n",
       "   507,\n",
       "   508,\n",
       "   509,\n",
       "   510,\n",
       "   511,\n",
       "   512,\n",
       "   513,\n",
       "   514,\n",
       "   515,\n",
       "   516,\n",
       "   517,\n",
       "   518,\n",
       "   519,\n",
       "   520,\n",
       "   521,\n",
       "   522,\n",
       "   523,\n",
       "   524,\n",
       "   525,\n",
       "   526,\n",
       "   527,\n",
       "   528,\n",
       "   529,\n",
       "   530,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   535,\n",
       "   536,\n",
       "   537,\n",
       "   538,\n",
       "   539,\n",
       "   540,\n",
       "   541,\n",
       "   542,\n",
       "   543,\n",
       "   544,\n",
       "   545,\n",
       "   546,\n",
       "   547,\n",
       "   548,\n",
       "   549,\n",
       "   550,\n",
       "   551,\n",
       "   552,\n",
       "   553,\n",
       "   554,\n",
       "   555,\n",
       "   556,\n",
       "   557,\n",
       "   558,\n",
       "   559,\n",
       "   560,\n",
       "   561,\n",
       "   562,\n",
       "   563,\n",
       "   564,\n",
       "   565,\n",
       "   566,\n",
       "   567,\n",
       "   568,\n",
       "   569,\n",
       "   570,\n",
       "   571,\n",
       "   572,\n",
       "   573,\n",
       "   574,\n",
       "   575,\n",
       "   576,\n",
       "   577,\n",
       "   578,\n",
       "   579,\n",
       "   580,\n",
       "   581,\n",
       "   582,\n",
       "   583,\n",
       "   584,\n",
       "   585,\n",
       "   586,\n",
       "   587,\n",
       "   588,\n",
       "   589,\n",
       "   590,\n",
       "   591,\n",
       "   592,\n",
       "   593,\n",
       "   594,\n",
       "   595,\n",
       "   596,\n",
       "   597,\n",
       "   598,\n",
       "   599,\n",
       "   600,\n",
       "   601,\n",
       "   602,\n",
       "   603,\n",
       "   604,\n",
       "   605,\n",
       "   606,\n",
       "   607,\n",
       "   608,\n",
       "   609,\n",
       "   610,\n",
       "   611,\n",
       "   612,\n",
       "   613,\n",
       "   614,\n",
       "   615,\n",
       "   616,\n",
       "   617,\n",
       "   618,\n",
       "   619,\n",
       "   620,\n",
       "   621,\n",
       "   622,\n",
       "   623,\n",
       "   624,\n",
       "   625,\n",
       "   626,\n",
       "   627,\n",
       "   628,\n",
       "   629,\n",
       "   630,\n",
       "   631,\n",
       "   632,\n",
       "   633,\n",
       "   634,\n",
       "   635,\n",
       "   636,\n",
       "   637,\n",
       "   638,\n",
       "   639,\n",
       "   640,\n",
       "   641,\n",
       "   642,\n",
       "   643,\n",
       "   644,\n",
       "   645,\n",
       "   646,\n",
       "   647,\n",
       "   648,\n",
       "   649,\n",
       "   650,\n",
       "   651,\n",
       "   652,\n",
       "   653,\n",
       "   654,\n",
       "   655,\n",
       "   656,\n",
       "   657,\n",
       "   658,\n",
       "   659,\n",
       "   660,\n",
       "   661,\n",
       "   662,\n",
       "   663,\n",
       "   664,\n",
       "   665,\n",
       "   666,\n",
       "   667,\n",
       "   668,\n",
       "   669,\n",
       "   670,\n",
       "   671,\n",
       "   672,\n",
       "   673,\n",
       "   674,\n",
       "   675,\n",
       "   676,\n",
       "   677,\n",
       "   678,\n",
       "   679,\n",
       "   680,\n",
       "   681,\n",
       "   682,\n",
       "   683,\n",
       "   684,\n",
       "   685,\n",
       "   686,\n",
       "   687,\n",
       "   688,\n",
       "   689,\n",
       "   690,\n",
       "   691,\n",
       "   692,\n",
       "   693,\n",
       "   694,\n",
       "   695,\n",
       "   696,\n",
       "   697,\n",
       "   698,\n",
       "   699,\n",
       "   700,\n",
       "   701,\n",
       "   702,\n",
       "   703,\n",
       "   704,\n",
       "   705,\n",
       "   706,\n",
       "   707,\n",
       "   708,\n",
       "   709,\n",
       "   710,\n",
       "   711,\n",
       "   712,\n",
       "   713,\n",
       "   714,\n",
       "   715,\n",
       "   716,\n",
       "   717,\n",
       "   718,\n",
       "   719,\n",
       "   720],\n",
       "  'TNS': 720,\n",
       "  'truncation_not_idx_minimal': array([], shape=(0, 2), dtype=float64),\n",
       "  'TNIM': 0,\n",
       "  'intercept_in_design': False,\n",
       "  'A_intercept_columns': 1,\n",
       "  'B_intercept_columns': 2,\n",
       "  'user_forced_truncation_not_idx': [1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   208,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   212,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   224,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   252,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   272,\n",
       "   273,\n",
       "   274,\n",
       "   275,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   280,\n",
       "   281,\n",
       "   282,\n",
       "   283,\n",
       "   284,\n",
       "   285,\n",
       "   286,\n",
       "   287,\n",
       "   288,\n",
       "   289,\n",
       "   290,\n",
       "   291,\n",
       "   292,\n",
       "   293,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   297,\n",
       "   298,\n",
       "   299,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   303,\n",
       "   304,\n",
       "   305,\n",
       "   306,\n",
       "   307,\n",
       "   308,\n",
       "   309,\n",
       "   310,\n",
       "   311,\n",
       "   312,\n",
       "   313,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   331,\n",
       "   332,\n",
       "   333,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   337,\n",
       "   338,\n",
       "   339,\n",
       "   340,\n",
       "   341,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   345,\n",
       "   346,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   350,\n",
       "   351,\n",
       "   352,\n",
       "   353,\n",
       "   354,\n",
       "   355,\n",
       "   356,\n",
       "   357,\n",
       "   358,\n",
       "   359,\n",
       "   360,\n",
       "   361,\n",
       "   362,\n",
       "   363,\n",
       "   364,\n",
       "   365,\n",
       "   366,\n",
       "   367,\n",
       "   368,\n",
       "   369,\n",
       "   370,\n",
       "   371,\n",
       "   372,\n",
       "   373,\n",
       "   374,\n",
       "   375,\n",
       "   376,\n",
       "   377,\n",
       "   378,\n",
       "   379,\n",
       "   380,\n",
       "   381,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   387,\n",
       "   388,\n",
       "   389,\n",
       "   390,\n",
       "   391,\n",
       "   392,\n",
       "   393,\n",
       "   394,\n",
       "   395,\n",
       "   396,\n",
       "   397,\n",
       "   398,\n",
       "   399,\n",
       "   400,\n",
       "   401,\n",
       "   402,\n",
       "   403,\n",
       "   404,\n",
       "   405,\n",
       "   406,\n",
       "   407,\n",
       "   408,\n",
       "   409,\n",
       "   410,\n",
       "   411,\n",
       "   412,\n",
       "   413,\n",
       "   414,\n",
       "   415,\n",
       "   416,\n",
       "   417,\n",
       "   418,\n",
       "   419,\n",
       "   420,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   424,\n",
       "   425,\n",
       "   426,\n",
       "   427,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   431,\n",
       "   432,\n",
       "   433,\n",
       "   434,\n",
       "   435,\n",
       "   436,\n",
       "   437,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   441,\n",
       "   442,\n",
       "   443,\n",
       "   444,\n",
       "   445,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   449,\n",
       "   450,\n",
       "   451,\n",
       "   452,\n",
       "   453,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   457,\n",
       "   458,\n",
       "   459,\n",
       "   460,\n",
       "   461,\n",
       "   462,\n",
       "   463,\n",
       "   464,\n",
       "   465,\n",
       "   466,\n",
       "   467,\n",
       "   468,\n",
       "   469,\n",
       "   470,\n",
       "   471,\n",
       "   472,\n",
       "   473,\n",
       "   474,\n",
       "   475,\n",
       "   476,\n",
       "   477,\n",
       "   478,\n",
       "   479,\n",
       "   480,\n",
       "   481,\n",
       "   482,\n",
       "   483,\n",
       "   484,\n",
       "   485,\n",
       "   486,\n",
       "   487,\n",
       "   488,\n",
       "   489,\n",
       "   490,\n",
       "   491,\n",
       "   492,\n",
       "   493,\n",
       "   494,\n",
       "   495,\n",
       "   496,\n",
       "   497,\n",
       "   498,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   502,\n",
       "   503,\n",
       "   504,\n",
       "   505,\n",
       "   506,\n",
       "   507,\n",
       "   508,\n",
       "   509,\n",
       "   510,\n",
       "   511,\n",
       "   512,\n",
       "   513,\n",
       "   514,\n",
       "   515,\n",
       "   516,\n",
       "   517,\n",
       "   518,\n",
       "   519,\n",
       "   520,\n",
       "   521,\n",
       "   522,\n",
       "   523,\n",
       "   524,\n",
       "   525,\n",
       "   526,\n",
       "   527,\n",
       "   528,\n",
       "   529,\n",
       "   530,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   535,\n",
       "   536,\n",
       "   537,\n",
       "   538,\n",
       "   539,\n",
       "   540,\n",
       "   541,\n",
       "   542,\n",
       "   543,\n",
       "   544,\n",
       "   545,\n",
       "   546,\n",
       "   547,\n",
       "   548,\n",
       "   549,\n",
       "   550,\n",
       "   551,\n",
       "   552,\n",
       "   553,\n",
       "   554,\n",
       "   555,\n",
       "   556,\n",
       "   557,\n",
       "   558,\n",
       "   559,\n",
       "   560,\n",
       "   561,\n",
       "   562,\n",
       "   563,\n",
       "   564,\n",
       "   565,\n",
       "   566,\n",
       "   567,\n",
       "   568,\n",
       "   569,\n",
       "   570,\n",
       "   571,\n",
       "   572,\n",
       "   573,\n",
       "   574,\n",
       "   575,\n",
       "   576,\n",
       "   577,\n",
       "   578,\n",
       "   579,\n",
       "   580,\n",
       "   581,\n",
       "   582,\n",
       "   583,\n",
       "   584,\n",
       "   585,\n",
       "   586,\n",
       "   587,\n",
       "   588,\n",
       "   589,\n",
       "   590,\n",
       "   591,\n",
       "   592,\n",
       "   593,\n",
       "   594,\n",
       "   595,\n",
       "   596,\n",
       "   597,\n",
       "   598,\n",
       "   599,\n",
       "   600,\n",
       "   601,\n",
       "   602,\n",
       "   603,\n",
       "   604,\n",
       "   605,\n",
       "   606,\n",
       "   607,\n",
       "   608,\n",
       "   609,\n",
       "   610,\n",
       "   611,\n",
       "   612,\n",
       "   613,\n",
       "   614,\n",
       "   615,\n",
       "   616,\n",
       "   617,\n",
       "   618,\n",
       "   619,\n",
       "   620,\n",
       "   621,\n",
       "   622,\n",
       "   623,\n",
       "   624,\n",
       "   625,\n",
       "   626,\n",
       "   627,\n",
       "   628,\n",
       "   629,\n",
       "   630,\n",
       "   631,\n",
       "   632,\n",
       "   633,\n",
       "   634,\n",
       "   635,\n",
       "   636,\n",
       "   637,\n",
       "   638,\n",
       "   639,\n",
       "   640,\n",
       "   641,\n",
       "   642,\n",
       "   643,\n",
       "   644,\n",
       "   645,\n",
       "   646,\n",
       "   647,\n",
       "   648,\n",
       "   649,\n",
       "   650,\n",
       "   651,\n",
       "   652,\n",
       "   653,\n",
       "   654,\n",
       "   655,\n",
       "   656,\n",
       "   657,\n",
       "   658,\n",
       "   659,\n",
       "   660,\n",
       "   661,\n",
       "   662,\n",
       "   663,\n",
       "   664,\n",
       "   665,\n",
       "   666,\n",
       "   667,\n",
       "   668,\n",
       "   669,\n",
       "   670,\n",
       "   671,\n",
       "   672,\n",
       "   673,\n",
       "   674,\n",
       "   675,\n",
       "   676,\n",
       "   677,\n",
       "   678,\n",
       "   679,\n",
       "   680,\n",
       "   681,\n",
       "   682,\n",
       "   683,\n",
       "   684,\n",
       "   685,\n",
       "   686,\n",
       "   687,\n",
       "   688,\n",
       "   689,\n",
       "   690,\n",
       "   691,\n",
       "   692,\n",
       "   693,\n",
       "   694,\n",
       "   695,\n",
       "   696,\n",
       "   697,\n",
       "   698,\n",
       "   699,\n",
       "   700,\n",
       "   701,\n",
       "   702,\n",
       "   703,\n",
       "   704,\n",
       "   705,\n",
       "   706,\n",
       "   707,\n",
       "   708,\n",
       "   709,\n",
       "   710,\n",
       "   711,\n",
       "   712,\n",
       "   713,\n",
       "   714,\n",
       "   715,\n",
       "   716,\n",
       "   717,\n",
       "   718,\n",
       "   719,\n",
       "   720],\n",
       "  'prior_prec_intercept': [5, 2],\n",
       "  'prior_prec_slope': [0, 0.6],\n",
       "  'prior_prec_sd': [20, 40],\n",
       "  'prior_mean_intercept': [0, 1],\n",
       "  'prior_mean_coefficients': [0, 1],\n",
       "  'exclude_priors': False},\n",
       " 'truncation_df2':          sample    type                                          phenotype  \\\n",
       " 0        10x_6K  benign  b_cell_macrophage_precursor_or_follicular_LTB_...   \n",
       " 1        10x_6K  benign                                    B_cell:immature   \n",
       " 2        10x_6K  benign                        B_cell:immature_IGLC3_IGLC2   \n",
       " 3        10x_6K  benign              B_cell:Memory_ITM2C_IGHA1_MZB1_JCHAIN   \n",
       " 4        10x_6K  benign                       Dendritic_CD11_CD1_high_mito   \n",
       " ..          ...     ...                                                ...   \n",
       " 715  SRR7244582  benign                   T_cell:CD8+_GZMK_DUSP2_LYAR_CCL5   \n",
       " 716  SRR7244582  benign                          T_cell:CD8+_non_activated   \n",
       " 717  SRR7244582  benign                              T_cell:CD8+_PPBP_SAT1   \n",
       " 718  SRR7244582  benign                                  T_cell:CD8+_S100B   \n",
       " 719  SRR7244582  benign                           T_cell:CD8low_TIMP1_PPBP   \n",
       " \n",
       "      count cell_group  proportion  \n",
       " 0       42         BM    0.008350  \n",
       " 1      361         B1    0.071769  \n",
       " 2       57         B2    0.011332  \n",
       " 3       40         B3    0.007952  \n",
       " 4       75         Dm    0.014911  \n",
       " ..     ...        ...         ...  \n",
       " 715    197      CD8 2    0.060727  \n",
       " 716    320      CD8 3    0.098644  \n",
       " 717     39      CD8 4    0.012022  \n",
       " 718     88      CD8 5    0.027127  \n",
       " 719    107      CD8 6    0.032984  \n",
       " \n",
       " [720 rows x 6 columns],\n",
       " 'sample': 'sample',\n",
       " 'cell_group': 'cell_group',\n",
       " 'count': 'count',\n",
       " 'formula_composition': 'count ~ 0 + type',\n",
       " 'formula_variability': 'count ~ 1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>MCSE</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>5%</th>\n",
       "      <th>50%</th>\n",
       "      <th>95%</th>\n",
       "      <th>N_Eff</th>\n",
       "      <th>N_Eff/s</th>\n",
       "      <th>R_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-478862.000000</td>\n",
       "      <td>0.204040</td>\n",
       "      <td>7.714380</td>\n",
       "      <td>-478875.000000</td>\n",
       "      <td>-478862.000000</td>\n",
       "      <td>-478850.000000</td>\n",
       "      <td>1429.46</td>\n",
       "      <td>158.793</td>\n",
       "      <td>1.002840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_raw_raw[1,1]</th>\n",
       "      <td>-0.851923</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.092617</td>\n",
       "      <td>-0.999387</td>\n",
       "      <td>-0.853771</td>\n",
       "      <td>-0.694325</td>\n",
       "      <td>4462.90</td>\n",
       "      <td>495.767</td>\n",
       "      <td>1.000720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_raw_raw[1,2]</th>\n",
       "      <td>-0.560180</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.114238</td>\n",
       "      <td>-0.743504</td>\n",
       "      <td>-0.562387</td>\n",
       "      <td>-0.373055</td>\n",
       "      <td>5152.17</td>\n",
       "      <td>572.336</td>\n",
       "      <td>0.999473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_raw_raw[1,3]</th>\n",
       "      <td>0.221579</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.103563</td>\n",
       "      <td>0.054934</td>\n",
       "      <td>0.222160</td>\n",
       "      <td>0.388933</td>\n",
       "      <td>5819.86</td>\n",
       "      <td>646.507</td>\n",
       "      <td>0.999705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_raw_raw[1,4]</th>\n",
       "      <td>0.696598</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.108433</td>\n",
       "      <td>0.516118</td>\n",
       "      <td>0.694070</td>\n",
       "      <td>0.877368</td>\n",
       "      <td>4987.11</td>\n",
       "      <td>554.000</td>\n",
       "      <td>0.999840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[716]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[717]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[718]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[719]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[720]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1731 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Mean      MCSE    StdDev             5%  \\\n",
       "lp__              -478862.000000  0.204040  7.714380 -478875.000000   \n",
       "beta_raw_raw[1,1]      -0.851923  0.001386  0.092617      -0.999387   \n",
       "beta_raw_raw[1,2]      -0.560180  0.001592  0.114238      -0.743504   \n",
       "beta_raw_raw[1,3]       0.221579  0.001358  0.103563       0.054934   \n",
       "beta_raw_raw[1,4]       0.696598  0.001535  0.108433       0.516118   \n",
       "...                          ...       ...       ...            ...   \n",
       "log_lik[716]            0.000000       NaN  0.000000       0.000000   \n",
       "log_lik[717]            0.000000       NaN  0.000000       0.000000   \n",
       "log_lik[718]            0.000000       NaN  0.000000       0.000000   \n",
       "log_lik[719]            0.000000       NaN  0.000000       0.000000   \n",
       "log_lik[720]            0.000000       NaN  0.000000       0.000000   \n",
       "\n",
       "                             50%            95%    N_Eff  N_Eff/s     R_hat  \n",
       "lp__              -478862.000000 -478850.000000  1429.46  158.793  1.002840  \n",
       "beta_raw_raw[1,1]      -0.853771      -0.694325  4462.90  495.767  1.000720  \n",
       "beta_raw_raw[1,2]      -0.562387      -0.373055  5152.17  572.336  0.999473  \n",
       "beta_raw_raw[1,3]       0.222160       0.388933  5819.86  646.507  0.999705  \n",
       "beta_raw_raw[1,4]       0.694070       0.877368  4987.11  554.000  0.999840  \n",
       "...                          ...            ...      ...      ...       ...  \n",
       "log_lik[716]            0.000000       0.000000      NaN      NaN       NaN  \n",
       "log_lik[717]            0.000000       0.000000      NaN      NaN       NaN  \n",
       "log_lik[718]            0.000000       0.000000      NaN      NaN       NaN  \n",
       "log_lik[719]            0.000000       0.000000      NaN      NaN       NaN  \n",
       "log_lik[720]            0.000000       0.000000      NaN      NaN       NaN  \n",
       "\n",
       "[1731 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.get('fit').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chain__</th>\n",
       "      <th>iter__</th>\n",
       "      <th>draw__</th>\n",
       "      <th>variable</th>\n",
       "      <th>C</th>\n",
       "      <th>M</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>beta</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.158520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>beta</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.171810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>beta</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>beta</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.249220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>beta</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.890231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>996.0</td>\n",
       "      <td>3996</td>\n",
       "      <td>beta</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>-2.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287996</th>\n",
       "      <td>4.0</td>\n",
       "      <td>997.0</td>\n",
       "      <td>3997</td>\n",
       "      <td>beta</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>-2.109030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287997</th>\n",
       "      <td>4.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>3998</td>\n",
       "      <td>beta</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>-2.491090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287998</th>\n",
       "      <td>4.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>3999</td>\n",
       "      <td>beta</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>-2.057070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287999</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>beta</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>-3.244830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        chain__  iter__  draw__ variable  C   M     value\n",
       "0           1.0     1.0       1     beta  1   1  1.158520\n",
       "1           1.0     2.0       2     beta  1   1  1.171810\n",
       "2           1.0     3.0       3     beta  1   1  0.990431\n",
       "3           1.0     4.0       4     beta  1   1  1.249220\n",
       "4           1.0     5.0       5     beta  1   1  0.890231\n",
       "...         ...     ...     ...      ... ..  ..       ...\n",
       "287995      4.0   996.0    3996     beta  2  36 -2.003280\n",
       "287996      4.0   997.0    3997     beta  2  36 -2.109030\n",
       "287997      4.0   998.0    3998     beta  2  36 -2.491090\n",
       "287998      4.0   999.0    3999     beta  2  36 -2.057070\n",
       "287999      4.0  1000.0    4000     beta  2  36 -3.244830\n",
       "\n",
       "[288000 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utilities import draws_to_tibble_x_y\n",
    "draws_to_tibble_x_y(res.get('fit'), 'beta', 'C', 'M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is for dev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res\n",
    "# Retrieve attributes from data\n",
    "cell_group = data.get(\"cell_group\", None)\n",
    "model_input = data.get(\"model_input\", {})\n",
    "fit = data.get(\"fit\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = draws_to_tibble_x_y(fit, \"beta\", \"C\", \"M\")\n",
    "beta = beta.pivot(index = ['chain__', 'iter__', 'draw__', 'variable', 'M'], columns='C', values='value')\n",
    "beta.columns = beta_factor_of_interest\n",
    "beta.reset_index(inplace=True)\n",
    "\n",
    "# Abundance\n",
    "draws = beta.drop(columns=['variable'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chain__</th>\n",
       "      <th>iter__</th>\n",
       "      <th>draw__</th>\n",
       "      <th>M</th>\n",
       "      <th>type[benign]</th>\n",
       "      <th>type[cancer]</th>\n",
       "      <th>type[cancer] - type[benign]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.158520</td>\n",
       "      <td>0.775575</td>\n",
       "      <td>-0.382945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.783222</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>-0.701921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.285722</td>\n",
       "      <td>-0.458924</td>\n",
       "      <td>-0.173202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.807143</td>\n",
       "      <td>-1.252030</td>\n",
       "      <td>-0.444887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.594940</td>\n",
       "      <td>0.536941</td>\n",
       "      <td>-0.057999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>32</td>\n",
       "      <td>0.612113</td>\n",
       "      <td>0.923040</td>\n",
       "      <td>0.310927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143996</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.302231</td>\n",
       "      <td>1.017630</td>\n",
       "      <td>0.715399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143997</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>34</td>\n",
       "      <td>0.428284</td>\n",
       "      <td>0.526152</td>\n",
       "      <td>0.097868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143998</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.892626</td>\n",
       "      <td>-0.814043</td>\n",
       "      <td>0.078583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143999</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>36</td>\n",
       "      <td>-1.613100</td>\n",
       "      <td>-3.244830</td>\n",
       "      <td>-1.631730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        chain__  iter__  draw__   M  type[benign]  type[cancer]  \\\n",
       "0           1.0     1.0       1   1      1.158520      0.775575   \n",
       "1           1.0     1.0       1   2      0.783222      0.081301   \n",
       "2           1.0     1.0       1   3     -0.285722     -0.458924   \n",
       "3           1.0     1.0       1   4     -0.807143     -1.252030   \n",
       "4           1.0     1.0       1   5      0.594940      0.536941   \n",
       "...         ...     ...     ...  ..           ...           ...   \n",
       "143995      4.0  1000.0    4000  32      0.612113      0.923040   \n",
       "143996      4.0  1000.0    4000  33      0.302231      1.017630   \n",
       "143997      4.0  1000.0    4000  34      0.428284      0.526152   \n",
       "143998      4.0  1000.0    4000  35     -0.892626     -0.814043   \n",
       "143999      4.0  1000.0    4000  36     -1.613100     -3.244830   \n",
       "\n",
       "        type[cancer] - type[benign]  \n",
       "0                         -0.382945  \n",
       "1                         -0.701921  \n",
       "2                         -0.173202  \n",
       "3                         -0.444887  \n",
       "4                         -0.057999  \n",
       "...                             ...  \n",
       "143995                     0.310927  \n",
       "143996                     0.715399  \n",
       "143997                     0.097868  \n",
       "143998                     0.078583  \n",
       "143999                    -1.631730  \n",
       "\n",
       "[144000 rows x 7 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utilities import mutate_from_expr_list\n",
    "mutate_from_expr_list(x = draws, formula_expr = contrasts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>C</th>\n",
       "      <th>M</th>\n",
       "      <th>Mean</th>\n",
       "      <th>5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "      <th>N_Eff</th>\n",
       "      <th>R_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.37351</td>\n",
       "      <td>4.83252</td>\n",
       "      <td>5.16844</td>\n",
       "      <td>5.38334</td>\n",
       "      <td>5.58229</td>\n",
       "      <td>5.89212</td>\n",
       "      <td>6364.62</td>\n",
       "      <td>0.999631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.03841</td>\n",
       "      <td>4.52449</td>\n",
       "      <td>4.82230</td>\n",
       "      <td>5.04565</td>\n",
       "      <td>5.25726</td>\n",
       "      <td>5.53418</td>\n",
       "      <td>6444.76</td>\n",
       "      <td>0.999632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.86026</td>\n",
       "      <td>5.28062</td>\n",
       "      <td>5.61926</td>\n",
       "      <td>5.86659</td>\n",
       "      <td>6.10663</td>\n",
       "      <td>6.43122</td>\n",
       "      <td>6001.91</td>\n",
       "      <td>0.999787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6.22116</td>\n",
       "      <td>5.56206</td>\n",
       "      <td>5.96072</td>\n",
       "      <td>6.22775</td>\n",
       "      <td>6.48845</td>\n",
       "      <td>6.85647</td>\n",
       "      <td>6021.29</td>\n",
       "      <td>0.999790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6.30294</td>\n",
       "      <td>5.64752</td>\n",
       "      <td>6.04536</td>\n",
       "      <td>6.31842</td>\n",
       "      <td>6.56848</td>\n",
       "      <td>6.91744</td>\n",
       "      <td>6128.24</td>\n",
       "      <td>1.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5.13514</td>\n",
       "      <td>4.60915</td>\n",
       "      <td>4.92109</td>\n",
       "      <td>5.13892</td>\n",
       "      <td>5.35392</td>\n",
       "      <td>5.64327</td>\n",
       "      <td>6314.97</td>\n",
       "      <td>1.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4.70984</td>\n",
       "      <td>4.16733</td>\n",
       "      <td>4.50470</td>\n",
       "      <td>4.71994</td>\n",
       "      <td>4.92534</td>\n",
       "      <td>5.21982</td>\n",
       "      <td>4878.97</td>\n",
       "      <td>0.999587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5.93214</td>\n",
       "      <td>5.19431</td>\n",
       "      <td>5.62673</td>\n",
       "      <td>5.92566</td>\n",
       "      <td>6.23012</td>\n",
       "      <td>6.67781</td>\n",
       "      <td>4842.18</td>\n",
       "      <td>1.000440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5.25147</td>\n",
       "      <td>4.73557</td>\n",
       "      <td>5.04467</td>\n",
       "      <td>5.25671</td>\n",
       "      <td>5.46140</td>\n",
       "      <td>5.74574</td>\n",
       "      <td>5978.11</td>\n",
       "      <td>0.999689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5.45960</td>\n",
       "      <td>4.86556</td>\n",
       "      <td>5.23682</td>\n",
       "      <td>5.45673</td>\n",
       "      <td>5.69644</td>\n",
       "      <td>6.03055</td>\n",
       "      <td>6876.81</td>\n",
       "      <td>0.999580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5.78488</td>\n",
       "      <td>5.16746</td>\n",
       "      <td>5.54861</td>\n",
       "      <td>5.78952</td>\n",
       "      <td>6.03098</td>\n",
       "      <td>6.37051</td>\n",
       "      <td>5866.45</td>\n",
       "      <td>0.999698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4.92660</td>\n",
       "      <td>4.40534</td>\n",
       "      <td>4.71253</td>\n",
       "      <td>4.93381</td>\n",
       "      <td>5.14276</td>\n",
       "      <td>5.43475</td>\n",
       "      <td>7227.85</td>\n",
       "      <td>0.999621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5.81310</td>\n",
       "      <td>5.23667</td>\n",
       "      <td>5.59202</td>\n",
       "      <td>5.82407</td>\n",
       "      <td>6.04572</td>\n",
       "      <td>6.34605</td>\n",
       "      <td>6307.93</td>\n",
       "      <td>0.999479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4.74237</td>\n",
       "      <td>4.26451</td>\n",
       "      <td>4.55774</td>\n",
       "      <td>4.75132</td>\n",
       "      <td>4.93483</td>\n",
       "      <td>5.19807</td>\n",
       "      <td>6465.67</td>\n",
       "      <td>0.999875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>6.32534</td>\n",
       "      <td>5.70541</td>\n",
       "      <td>6.08496</td>\n",
       "      <td>6.33188</td>\n",
       "      <td>6.57938</td>\n",
       "      <td>6.91271</td>\n",
       "      <td>5574.58</td>\n",
       "      <td>0.999304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5.51703</td>\n",
       "      <td>4.94815</td>\n",
       "      <td>5.29841</td>\n",
       "      <td>5.52637</td>\n",
       "      <td>5.74596</td>\n",
       "      <td>6.04859</td>\n",
       "      <td>6376.09</td>\n",
       "      <td>0.999622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5.33303</td>\n",
       "      <td>4.76022</td>\n",
       "      <td>5.10040</td>\n",
       "      <td>5.33800</td>\n",
       "      <td>5.56633</td>\n",
       "      <td>5.88769</td>\n",
       "      <td>6284.67</td>\n",
       "      <td>0.999844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>5.42648</td>\n",
       "      <td>4.89962</td>\n",
       "      <td>5.21508</td>\n",
       "      <td>5.43472</td>\n",
       "      <td>5.63851</td>\n",
       "      <td>5.93124</td>\n",
       "      <td>6364.78</td>\n",
       "      <td>0.999894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>5.81557</td>\n",
       "      <td>5.23751</td>\n",
       "      <td>5.58518</td>\n",
       "      <td>5.82538</td>\n",
       "      <td>6.05009</td>\n",
       "      <td>6.35058</td>\n",
       "      <td>6353.25</td>\n",
       "      <td>0.999905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>5.49282</td>\n",
       "      <td>4.94788</td>\n",
       "      <td>5.28085</td>\n",
       "      <td>5.50217</td>\n",
       "      <td>5.71819</td>\n",
       "      <td>5.99970</td>\n",
       "      <td>5239.09</td>\n",
       "      <td>0.999692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5.37539</td>\n",
       "      <td>4.76254</td>\n",
       "      <td>5.12321</td>\n",
       "      <td>5.37668</td>\n",
       "      <td>5.62439</td>\n",
       "      <td>5.97812</td>\n",
       "      <td>5519.26</td>\n",
       "      <td>0.999302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>5.26585</td>\n",
       "      <td>4.72016</td>\n",
       "      <td>5.05556</td>\n",
       "      <td>5.27033</td>\n",
       "      <td>5.49015</td>\n",
       "      <td>5.77839</td>\n",
       "      <td>6675.61</td>\n",
       "      <td>0.999186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>5.93049</td>\n",
       "      <td>5.26190</td>\n",
       "      <td>5.65343</td>\n",
       "      <td>5.92681</td>\n",
       "      <td>6.20617</td>\n",
       "      <td>6.62099</td>\n",
       "      <td>6485.43</td>\n",
       "      <td>0.999323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>5.12860</td>\n",
       "      <td>4.57791</td>\n",
       "      <td>4.90612</td>\n",
       "      <td>5.12922</td>\n",
       "      <td>5.35184</td>\n",
       "      <td>5.67668</td>\n",
       "      <td>5709.61</td>\n",
       "      <td>1.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.48180</td>\n",
       "      <td>2.68442</td>\n",
       "      <td>3.16335</td>\n",
       "      <td>3.49452</td>\n",
       "      <td>3.81162</td>\n",
       "      <td>4.23480</td>\n",
       "      <td>1788.69</td>\n",
       "      <td>1.004960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>5.33439</td>\n",
       "      <td>4.74869</td>\n",
       "      <td>5.11248</td>\n",
       "      <td>5.34039</td>\n",
       "      <td>5.56463</td>\n",
       "      <td>5.88694</td>\n",
       "      <td>5373.89</td>\n",
       "      <td>1.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>5.60255</td>\n",
       "      <td>5.04617</td>\n",
       "      <td>5.38145</td>\n",
       "      <td>5.61053</td>\n",
       "      <td>5.83524</td>\n",
       "      <td>6.12882</td>\n",
       "      <td>7188.38</td>\n",
       "      <td>1.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>5.66756</td>\n",
       "      <td>5.05934</td>\n",
       "      <td>5.42475</td>\n",
       "      <td>5.67302</td>\n",
       "      <td>5.92004</td>\n",
       "      <td>6.27996</td>\n",
       "      <td>5528.09</td>\n",
       "      <td>1.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>5.38611</td>\n",
       "      <td>4.85813</td>\n",
       "      <td>5.18392</td>\n",
       "      <td>5.39985</td>\n",
       "      <td>5.59712</td>\n",
       "      <td>5.89179</td>\n",
       "      <td>5975.06</td>\n",
       "      <td>0.999565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.42118</td>\n",
       "      <td>4.85727</td>\n",
       "      <td>5.20617</td>\n",
       "      <td>5.43176</td>\n",
       "      <td>5.65233</td>\n",
       "      <td>5.94112</td>\n",
       "      <td>6405.16</td>\n",
       "      <td>0.999974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>6.32927</td>\n",
       "      <td>5.71146</td>\n",
       "      <td>6.08193</td>\n",
       "      <td>6.32838</td>\n",
       "      <td>6.58516</td>\n",
       "      <td>6.95114</td>\n",
       "      <td>5690.62</td>\n",
       "      <td>0.999280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5.19591</td>\n",
       "      <td>4.61531</td>\n",
       "      <td>4.98349</td>\n",
       "      <td>5.20526</td>\n",
       "      <td>5.41782</td>\n",
       "      <td>5.72909</td>\n",
       "      <td>6802.17</td>\n",
       "      <td>0.999417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>5.92562</td>\n",
       "      <td>5.38685</td>\n",
       "      <td>5.69963</td>\n",
       "      <td>5.93315</td>\n",
       "      <td>6.16055</td>\n",
       "      <td>6.46197</td>\n",
       "      <td>5318.19</td>\n",
       "      <td>1.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>5.55524</td>\n",
       "      <td>5.00759</td>\n",
       "      <td>5.34261</td>\n",
       "      <td>5.56588</td>\n",
       "      <td>5.77760</td>\n",
       "      <td>6.07929</td>\n",
       "      <td>6497.37</td>\n",
       "      <td>0.999425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>5.74240</td>\n",
       "      <td>5.11607</td>\n",
       "      <td>5.49189</td>\n",
       "      <td>5.73996</td>\n",
       "      <td>5.99784</td>\n",
       "      <td>6.36253</td>\n",
       "      <td>5687.56</td>\n",
       "      <td>0.999582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>alpha_normalised</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>3.41356</td>\n",
       "      <td>2.60894</td>\n",
       "      <td>3.09525</td>\n",
       "      <td>3.42349</td>\n",
       "      <td>3.75006</td>\n",
       "      <td>4.16829</td>\n",
       "      <td>1084.50</td>\n",
       "      <td>1.003730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            variable  C   M     Mean       5%      25%      50%      75%  \\\n",
       "0   alpha_normalised  1   1  5.37351  4.83252  5.16844  5.38334  5.58229   \n",
       "1   alpha_normalised  1   2  5.03841  4.52449  4.82230  5.04565  5.25726   \n",
       "2   alpha_normalised  1   3  5.86026  5.28062  5.61926  5.86659  6.10663   \n",
       "3   alpha_normalised  1   4  6.22116  5.56206  5.96072  6.22775  6.48845   \n",
       "4   alpha_normalised  1   5  6.30294  5.64752  6.04536  6.31842  6.56848   \n",
       "5   alpha_normalised  1   6  5.13514  4.60915  4.92109  5.13892  5.35392   \n",
       "6   alpha_normalised  1   7  4.70984  4.16733  4.50470  4.71994  4.92534   \n",
       "7   alpha_normalised  1   8  5.93214  5.19431  5.62673  5.92566  6.23012   \n",
       "8   alpha_normalised  1   9  5.25147  4.73557  5.04467  5.25671  5.46140   \n",
       "9   alpha_normalised  1  10  5.45960  4.86556  5.23682  5.45673  5.69644   \n",
       "10  alpha_normalised  1  11  5.78488  5.16746  5.54861  5.78952  6.03098   \n",
       "11  alpha_normalised  1  12  4.92660  4.40534  4.71253  4.93381  5.14276   \n",
       "12  alpha_normalised  1  13  5.81310  5.23667  5.59202  5.82407  6.04572   \n",
       "13  alpha_normalised  1  14  4.74237  4.26451  4.55774  4.75132  4.93483   \n",
       "14  alpha_normalised  1  15  6.32534  5.70541  6.08496  6.33188  6.57938   \n",
       "15  alpha_normalised  1  16  5.51703  4.94815  5.29841  5.52637  5.74596   \n",
       "16  alpha_normalised  1  17  5.33303  4.76022  5.10040  5.33800  5.56633   \n",
       "17  alpha_normalised  1  18  5.42648  4.89962  5.21508  5.43472  5.63851   \n",
       "18  alpha_normalised  1  19  5.81557  5.23751  5.58518  5.82538  6.05009   \n",
       "19  alpha_normalised  1  20  5.49282  4.94788  5.28085  5.50217  5.71819   \n",
       "20  alpha_normalised  1  21  5.37539  4.76254  5.12321  5.37668  5.62439   \n",
       "21  alpha_normalised  1  22  5.26585  4.72016  5.05556  5.27033  5.49015   \n",
       "22  alpha_normalised  1  23  5.93049  5.26190  5.65343  5.92681  6.20617   \n",
       "23  alpha_normalised  1  24  5.12860  4.57791  4.90612  5.12922  5.35184   \n",
       "24  alpha_normalised  1  25  3.48180  2.68442  3.16335  3.49452  3.81162   \n",
       "25  alpha_normalised  1  26  5.33439  4.74869  5.11248  5.34039  5.56463   \n",
       "26  alpha_normalised  1  27  5.60255  5.04617  5.38145  5.61053  5.83524   \n",
       "27  alpha_normalised  1  28  5.66756  5.05934  5.42475  5.67302  5.92004   \n",
       "28  alpha_normalised  1  29  5.38611  4.85813  5.18392  5.39985  5.59712   \n",
       "29  alpha_normalised  1  30  5.42118  4.85727  5.20617  5.43176  5.65233   \n",
       "30  alpha_normalised  1  31  6.32927  5.71146  6.08193  6.32838  6.58516   \n",
       "31  alpha_normalised  1  32  5.19591  4.61531  4.98349  5.20526  5.41782   \n",
       "32  alpha_normalised  1  33  5.92562  5.38685  5.69963  5.93315  6.16055   \n",
       "33  alpha_normalised  1  34  5.55524  5.00759  5.34261  5.56588  5.77760   \n",
       "34  alpha_normalised  1  35  5.74240  5.11607  5.49189  5.73996  5.99784   \n",
       "35  alpha_normalised  1  36  3.41356  2.60894  3.09525  3.42349  3.75006   \n",
       "\n",
       "        95%    N_Eff     R_hat  \n",
       "0   5.89212  6364.62  0.999631  \n",
       "1   5.53418  6444.76  0.999632  \n",
       "2   6.43122  6001.91  0.999787  \n",
       "3   6.85647  6021.29  0.999790  \n",
       "4   6.91744  6128.24  1.000220  \n",
       "5   5.64327  6314.97  1.000090  \n",
       "6   5.21982  4878.97  0.999587  \n",
       "7   6.67781  4842.18  1.000440  \n",
       "8   5.74574  5978.11  0.999689  \n",
       "9   6.03055  6876.81  0.999580  \n",
       "10  6.37051  5866.45  0.999698  \n",
       "11  5.43475  7227.85  0.999621  \n",
       "12  6.34605  6307.93  0.999479  \n",
       "13  5.19807  6465.67  0.999875  \n",
       "14  6.91271  5574.58  0.999304  \n",
       "15  6.04859  6376.09  0.999622  \n",
       "16  5.88769  6284.67  0.999844  \n",
       "17  5.93124  6364.78  0.999894  \n",
       "18  6.35058  6353.25  0.999905  \n",
       "19  5.99970  5239.09  0.999692  \n",
       "20  5.97812  5519.26  0.999302  \n",
       "21  5.77839  6675.61  0.999186  \n",
       "22  6.62099  6485.43  0.999323  \n",
       "23  5.67668  5709.61  1.000460  \n",
       "24  4.23480  1788.69  1.004960  \n",
       "25  5.88694  5373.89  1.000010  \n",
       "26  6.12882  7188.38  1.000250  \n",
       "27  6.27996  5528.09  1.000010  \n",
       "28  5.89179  5975.06  0.999565  \n",
       "29  5.94112  6405.16  0.999974  \n",
       "30  6.95114  5690.62  0.999280  \n",
       "31  5.72909  6802.17  0.999417  \n",
       "32  6.46197  5318.19  1.000420  \n",
       "33  6.07929  6497.37  0.999425  \n",
       "34  6.36253  5687.56  0.999582  \n",
       "35  4.16829  1084.50  1.003730  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utilities import summary_to_tibble\n",
    "summary_to_tibble(fit, \"alpha_normalised\", \"C\", \"M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
